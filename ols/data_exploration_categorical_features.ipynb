{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import sklearn as sk\n",
    "import sklearn.preprocessing\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import statsmodels.formula.api as smapi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sys.path.insert(1, os.path.join(sys.path[0], '..'))  # add parent directory to path\n",
    "import samlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sale price distribution\n",
    "First step is to look at the target sale price for the training data set, i.e. the column we're trying to predict. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "target = pd.read_csv('../data/train_target.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "target.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sale price is in hte hundreds of thousands, so let's divide the price by 1000 to get more manageable numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "target = target / 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "logtarget = np.log1p(target)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge the training and test datasets for data preparation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def read():\n",
    "    \"\"\"Read training and test data and return a dataframe with ['Dataset','Id'] multi-index\n",
    "    \"\"\"\n",
    "    raw_train = pd.read_csv('../data/train_prepared_light.csv')\n",
    "    raw_test = pd.read_csv('../data/test_prepared_light.csv')\n",
    "    df = pd.concat([raw_train, raw_test], keys=['train', 'test'])\n",
    "    df.index.names = 'Dataset', 'Id'\n",
    "    return df\n",
    "    \n",
    "df = read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pp = samlib.Pipeline(df.copy())  \n",
    "assert pp == df  # the pipeline output equals df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def select_categorical_features(df):\n",
    "    return df.loc[:, df.dtypes == object]\n",
    "\n",
    "pp.append(select_categorical_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categorical features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've got 42 categorical features. We can use the `describe` method to get some statistics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pp().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pp().describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pp().isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Number of categories per feature **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 10))\n",
    "pp().describe().loc['unique'].sort_values(ascending=False).plot(kind='barh')\n",
    "plt.title('Number of categories per feature')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Number of nulls per feature **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nulls = pp().isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 10))\n",
    "ax = nulls[nulls > 0].sort_values(ascending=False).plot(kind='barh')\n",
    "plt.title('Number of nulls per feature')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But that's a lot of numbers to digest. Better get started plotting! To help with plotting, but also to improve linear regression models, we're going to standardize our data. But before that we must deal with the NaN values.\n",
    "http://sebastianraschka.com/Articles/2014_about_feature_scaling.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deal with NaN values \n",
    "Based on the descriptions, most of the null values are not missing values but an extra \"none\" category (e.g. no pool, or no alley etc). In the above graph, this is true for all features starting from 'BsmtFinType1'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def replace_bad_nulls(df):\n",
    "    nulls = df.isnull().sum()\n",
    "    bad_nulls_colz = nulls[nulls > 0].sort_values()['BsmtFinType1':].index\n",
    "    return samlib.fillna(df, lambda x: 'none', bad_nulls_colz)\n",
    "\n",
    "pp.append(replace_bad_nulls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Replace true nulls with mode **\n",
    "(work in progress)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def fill_nulls_with_mode(df):\n",
    "    return samlib.fillna(df, lambda dg: dg.mode().loc[0])\n",
    "pp.append(fill_nulls_with_mode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explore categories\n",
    "See http://seaborn.pydata.org/tutorial/categorical.html for some ideas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot value counts \n",
    "Gives an idea of the distribution of values for each categorical variable. We can see that some categories, such as 'Condition2', are almost constant so are unlikely to have a large impact on predicting the sale price. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = pp()\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plot_value_counts(data=df, **kwargs):\n",
    "    return sns.countplot(y=data.columns[0], data=data, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Sort by number of values\n",
    "colz_ordered_by_unique = df.describe().loc['unique'].sort_values(ascending=False).index\n",
    "df = df.reindex_axis(colz_ordered_by_unique, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "samlib.featureplot(df, nrows=7, ncols=3, figsize=(12, 4), plotfunc=plot_value_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot sale price distribution for each category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.loc['train', 'LogSalePrice'] = logtarget.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train = df.loc['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plot_price_dist(data, train=train, **kwargs):\n",
    "    \"\"\"Plot the price distribution for each category\"\"\"\n",
    "    col = data.columns[0]\n",
    "    dg = data.copy()\n",
    "    dg['LogSalePrice'] = train['LogSalePrice']\n",
    "    # Order categories by mean value\n",
    "    ordered_cats =  dg.groupby(col).median().sort_values('LogSalePrice', ascending=False).index\n",
    "    sns.violinplot(x=\"LogSalePrice\", y=col, data=dg, order=ordered_cats, scale='width', **kwargs)\n",
    "    #sns.swarmplot(x=\"LogSalePrice\", y=col, data=df, color=\"w\", alpha=.5);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "samlib.featureplot(train, nrows=7, ncols=3, figsize=(12, 8), plotfunc=plot_price_dist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Change dtypes to `category`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def object_to_category(df):\n",
    "    new = df.copy()\n",
    "    for column, ctype in zip(df.columns, df.dtypes):\n",
    "        if ctype == object:\n",
    "            new[column] = df[column].astype('category')\n",
    "    return new\n",
    "\n",
    "pp.append(object_to_category)\n",
    "assert all(pp().dtypes == 'category')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create dummy variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pd.get_dummies(pp())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
