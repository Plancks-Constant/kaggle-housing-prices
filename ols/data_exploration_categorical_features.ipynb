{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import sklearn as sk\n",
    "import sklearn.preprocessing\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import statsmodels.formula.api as smapi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sys.path.insert(1, os.path.join(sys.path[0], '..'))  # add parent directory to path\n",
    "import samlib\n",
    "imp.reload(samlib);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(level=logging.DEBUG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sale price distribution\n",
    "First step is to look at the target sale price for the training data set, i.e. the column we're trying to predict. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "target = pd.read_csv('../data/train_target.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "target.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sale price is in hte hundreds of thousands, so let's divide the price by 1000 to get more manageable numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "target = target / 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "logtarget = np.log1p(target)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge the training and test datasets for data preparation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def read():\n",
    "    \"\"\"Read training and test data and return a dataframe with ['Dataset','Id'] multi-index\n",
    "    \"\"\"\n",
    "    raw_train = pd.read_csv('../data/train_prepared_light.csv')\n",
    "    raw_test = pd.read_csv('../data/test_prepared_light.csv')\n",
    "    df = pd.concat([raw_train, raw_test], keys=['train', 'test'])\n",
    "    df.index.names = 'Dataset', 'Id'\n",
    "    return df\n",
    "    \n",
    "df = read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pp = samlib.Pipeline(df.copy())  \n",
    "assert pp == df  # the pipeline output equals df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def select_categorical_features(df):\n",
    "    return df.loc[:, df.dtypes == object]\n",
    "\n",
    "pp.append(select_categorical_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categorical features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've got 42 categorical features. We can use the `describe` method to get some statistics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pp().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pp().describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pp().isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Number of categories per feature **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 10))\n",
    "pp().describe().loc['unique'].sort_values(ascending=False).plot(kind='barh')\n",
    "plt.title('Number of categories per feature')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Number of nulls per feature **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nulls = pp().isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 10))\n",
    "ax = nulls[nulls > 0].sort_values(ascending=False).plot(kind='barh')\n",
    "plt.title('Number of nulls per feature')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But that's a lot of numbers to digest. Better get started plotting! To help with plotting, but also to improve linear regression models, we're going to standardize our data. But before that we must deal with the NaN values.\n",
    "http://sebastianraschka.com/Articles/2014_about_feature_scaling.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deal with NaN values \n",
    "Based on the descriptions, most of the null values are not missing values but an extra \"none\" category (e.g. no pool, or no alley etc). In the above graph, this is true for all features starting from 'BsmtFinType1'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def replace_bad_nulls(df):\n",
    "    nulls = df.isnull().sum()\n",
    "    bad_nulls_colz = nulls[nulls > 0].sort_values()['BsmtFinType1':].index\n",
    "    return samlib.fillna(df, lambda x: 'none', bad_nulls_colz)\n",
    "\n",
    "pp.append(replace_bad_nulls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Replace true nulls with mode **\n",
    "(work in progress)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def fill_nulls_with_mode(df):\n",
    "    return samlib.fillna(df, lambda dg: dg.mode().loc[0])\n",
    "pp.append(fill_nulls_with_mode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Change dtypes to `category`\n",
    "And *order* categories by the mean price, so the plots are easier to read."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pp()['LotShape'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def ordered_categories(series):\n",
    "    dg = pd.DataFrame(series).copy()\n",
    "    dg.loc['train','LogSalePrice'] = logtarget.values\n",
    "    return dg.groupby(dg.columns[0]).median().sort_values('LogSalePrice', ascending=False)\n",
    "\n",
    "ordered_categories(pp()['LotShape'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def categorize(feature):\n",
    "    feature = feature.copy()\n",
    "    return pd.Categorical(feature, ordered_categories(feature).index)\n",
    "\n",
    "categorize(pp()['LotShape'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def objects_to_categories(df):\n",
    "    return df.apply(categorize)\n",
    "\n",
    "objects_to_categories(pp())['LotShape'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pp.append(objects_to_categories)\n",
    "pp()['LotShape'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Order columns by uniques\n",
    "That way we see plots with more categories first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def order_columns_by_uniques(df):\n",
    "    colz_ordered_by_unique = df.describe().loc['unique'].sort_values(ascending=False).index\n",
    "    return df.reindex_axis(colz_ordered_by_unique, axis=1)\n",
    "\n",
    "pp.append(order_columns_by_uniques)\n",
    "pp().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explore categories\n",
    "See http://seaborn.pydata.org/tutorial/categorical.html for some ideas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = pp()\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot sale price distribution for each category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train = pp().loc['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plot_price_dist(y='LotShape', data=train, logtarget=logtarget, **kwargs):\n",
    "    \"\"\"Plot the price distribution for each category\"\"\"\n",
    "    dg = data[[y]].copy()\n",
    "    dg['LogSalePrice'] = logtarget\n",
    "    # Order categories by mean value\n",
    "    sns.violinplot(x=\"LogSalePrice\", y=y, data=dg, scale='width', **kwargs)\n",
    "    #sns.swarmplot(x=\"LogSalePrice\", y=y, data=dg, color=\"w\", alpha=.5);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plot_value_counts(y=None, data=df, **kwargs):\n",
    "    if y is None:\n",
    "        if data.shape[1] == 1:\n",
    "            y = data.columns[0]\n",
    "        else:\n",
    "            raise ValueError('Must pass y or a dataframe with a single column')\n",
    "    return sns.countplot(y=y, data=data, **kwargs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use `samlib.featureplots` to plot the distribution and value count of each category. \n",
    "\n",
    "Gives an idea of the distribution of values for each categorical variable. We can see that some categories, such as 'Condition2', are almost constant so are unlikely to have a large impact on predicting the sale xprice. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "samlib.featureplots(train, nrows=21, ncols=4, figsize=(2, 8), plotfuncs=(plot_price_dist, plot_value_counts), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create an indicator to detect the bad features, that:\n",
    "- are nearly constant (almost all values are in the same category)\n",
    "- the medians of the categories is nearly constant (unlikely to help predict the price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "looks_good = 'Neighborhood'\n",
    "looks_bad = 'Condition1'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The medians of the logprice within each categories should have maximum variance (the less the variance, the less we can distinguish the price within each category)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# medians\n",
    "med_price = logtarget.median().values\n",
    "df = pp()\n",
    "sharps = df.apply(lambda col: ordered_categories(col).std().values).iloc[0] / med_price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1, figsize=(12, 12))\n",
    "sharps.sort_values().plot(kind='barh', ax=ax)\n",
    "plt.title('Std of median logprices for each category (high is better)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The entropy of the value counts should be large: the higher the entropy, the more uniformly distributed the value counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from scipy.stats import entropy\n",
    "df = pp()\n",
    "unicounts = df.apply(lambda ser: entropy(ser.value_counts()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1, figsize=(12, 12))\n",
    "unicounts.sort_values().plot(kind='barh', ax=ax)\n",
    "plt.title('Entropy of value counts for each category (high is better)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Good features have a high variability and more uniform counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "goodness = sharps * unicounts    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1, figsize=(12, 12))\n",
    "goodness.sort_values().plot(kind='barh', ax=ax)\n",
    "plt.title('Goodness of category (high is better)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "topcolz = goodness.sort_values(ascending=False)[:12].index\n",
    "topcolz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot our top features and check that they indeed appear helpful (good variability in the medians and high entropy in the counts)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "samlib.featureplots(train[topcolz], nrows=6, ncols=4, figsize=(2, 8), plotfuncs=(plot_price_dist, plot_value_counts), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sort columns by goodness and save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = pp()\n",
    "df.columns = goodness.sort_values(ascending=False).index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.to_csv('cleaned_categorical_vars_with_colz_sorted_by_goodness.csv', index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
