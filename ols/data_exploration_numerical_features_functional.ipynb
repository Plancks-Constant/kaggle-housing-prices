{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import sklearn as sk\n",
    "import sklearn.preprocessing\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import statsmodels.formula.api as smapi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sys.path.insert(1, os.path.join(sys.path[0], '..'))  # add parent directory to path\n",
    "import samlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the cleaner chaining method for transforming the data https://tomaugspurger.github.io/method-chaining.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sale price distribution\n",
    "First step is to look at the target sale price for the training data set, i.e. the column we're trying to predict. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "target = pd.read_csv('../data/train_target.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "target.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sale price is in hte hundreds of thousands, so let's divide the price by 1000 to get more manageable numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "target = target / 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sns.distplot(target);\n",
    "plt.title('SalePrice')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import scipy as sp\n",
    "sp.stats.skew(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sp.stats.skewtest(target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The distribution is skewed (as demonstrated by the large z-score (and small pvalue) of teh skewtest). It is right skewed (the skew is positive). Skewed distribution are not ideal for linear models, which often assume a normal distribution. One way to correct for right-skewness is to take the log [1,2,3]\n",
    "\n",
    "- [1] http://fmwww.bc.edu/repec/bocode/t/transint.html \n",
    "- [2] https://www.r-statistics.com/2013/05/log-transformations-for-skewed-and-wide-distributions-from-practical-data-science-with-r/\n",
    "- [3] Alexandru Papiu's notebook https://www.kaggle.com/apapiu/house-prices-advanced-regression-techniques/regularized-linear-models/commentsnotebook "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We apply the function $x \\rightarrow \\log(1 + x)$ because it is always positive for $x \\geq 0$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "logtarget = np.log1p(target)\n",
    "print('skewness of logtarget = ', sp.stats.skew(logtarget)[0])\n",
    "print('skewness test of logtarget = ', sp.stats.skewtest(logtarget))\n",
    "sns.distplot(logtarget)\n",
    "plt.title(r'log(1 + SalePrice)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge the training and test datasets for data preparation\n",
    "We're going to explore the training dataset and apply some transformations to it (fixing missing values, transforming columns etc). We'll need to apply the same transformations to the test dataset. To make that easy, let's put the training and test datasets into one dataframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create a pipeline so we can process all the data later in one go if needed\n",
    "class Pipeline:\n",
    "    \n",
    "    def __init__(self, df):\n",
    "        self.input_df = df\n",
    "        self._pipeline = []\n",
    "        \n",
    "    def append(self, func):\n",
    "        \"\"\"Append function to pipe\"\"\"\n",
    "        if not self._pipeline or not self._same_func(self._pipeline[-1], func):\n",
    "            self._pipeline.append(func) \n",
    "    \n",
    "    def run(self):\n",
    "        \"\"\"Run the pipe\"\"\"\n",
    "        df = self.input_df\n",
    "        for func in self._pipeline:\n",
    "            df = df.pipe(func)\n",
    "        return df\n",
    "    \n",
    "    @staticmethod\n",
    "    def _same_func(f1, f2):\n",
    "        return f1.__name__ == f2.__name__\n",
    "    \n",
    "    def __str__(self):\n",
    "        return str(self._pipeline)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return repr(self._pipeline)\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def read():\n",
    "    \"\"\"Read training and test data and return a dataframe with ['Dataset','Id'] multi-index\n",
    "    \"\"\"\n",
    "    raw_train = pd.read_csv('../data/train_prepared_light.csv')\n",
    "    raw_test = pd.read_csv('../data/test_prepared_light.csv')\n",
    "    df = pd.concat([raw_train, raw_test], keys=['train', 'test'])\n",
    "    df.index.names = 'Dataset', 'Id'\n",
    "    return df\n",
    "    \n",
    "df = read()\n",
    "pipeline = Pipeline(df.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ncategories = sum(df.dtypes == object)\n",
    "ncategories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select Numerical features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset is wide with 78 features. Create dataframe containing the numerical features only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.columns, len(df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've got 3 data types: int, float and object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.dtypes.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the data between categorical and numerical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "is_categorical = (df.dtypes == object)\n",
    "is_numerical = ~is_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dfnum = df.loc[:, is_numerical].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dfnum.columns, len(dfnum.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've got 36 numerical features. We can use the `describe` method to get some statistics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "desc = dfnum.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But that's a lot of numbers to digest. Better get started plotting! To help with plotting, but also to improve linear regression models, we're going to standardize our data. But before that we must deal with the NaN values.\n",
    "http://sebastianraschka.com/Articles/2014_about_feature_scaling.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def select_numerical_features(df):\n",
    "    return df.loc[:, df.dtypes != object]\n",
    "\n",
    "pipeline.append(select_numerical_features)\n",
    "# Check the pipline\n",
    "all(pipeline.run() == dfnum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deal with NaN values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cols_with_nulls = dfnum.columns[dfnum.isnull().sum() > 0]\n",
    "cols_with_nulls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dfnum.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dfnum[cols_with_nulls].isnull().sum().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the description, the null values for the `MasVnrArea` should be 0 (no massonry veneer type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# We may want to refine this in the future. Perhaps build a model to predict the missing GarageCars from the other features?\n",
    "median_list = 'LotFrontage', 'BsmtFullBath','BsmtHalfBath', 'GarageCars', 'GarageArea'\n",
    "zero_list = 'MasVnrArea', 'BsmtFinSF1', 'BsmtFinSF2', 'TotalBsmtSF', 'BsmtUnfSF'\n",
    "\n",
    "def fillnans(dfnum):\n",
    "    return dfnum.pipe(samlib.fillna, 'median', median_list)\\\n",
    "     .pipe(samlib.fillna, lambda df: 0, zero_list)\\\n",
    "     .assign(GarageYrBlt=dfnum.GarageYrBlt.\n",
    "             fillna(dfnum.YearBuilt[dfnum.GarageYrBlt.isnull()]))  # fill with year garage was built\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dfnum = fillnans(dfnum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Check that we got rid of the nulls\n",
    "assert not samlib.has_nulls(dfnum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pipeline.append(fillnans)\n",
    "# Check the pipline\n",
    "pipeline, all(pipeline.run() == dfnum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot violinplots for each feature \n",
    "The violin plots give us some idea of the distribution of data for each feature. We can look for things like skewness, non-normality, and the presence of outliers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "\n",
    "def violinplot(df, ax=None):\n",
    "    if ax is None:\n",
    "        ax = plt.gca()\n",
    "    sns.violinplot(data=df, ax=ax)\n",
    "    for xlab in ax.get_xticklabels():\n",
    "        xlab.set_rotation(0)\n",
    "        \n",
    "\n",
    "def featureplot(df, nrows=1, ncols=1, figsize=(12,8), plotfunc=violinplot):\n",
    "    \"\"\"Plot the dataframe features\"\"\"\n",
    "    width, height = figsize\n",
    "    fig, axes = plt.subplots(nrows, ncols, figsize=(width, height * nrows));\n",
    "    i = 0\n",
    "    plots_per_figure = max(df.shape[1] // (nrows * ncols), 1)\n",
    "    if nrows == 1 and ncols == 1:\n",
    "        axes = [axes]\n",
    "    if nrows > 1 and ncols > 1:\n",
    "        axes = chain.from_iterable(axes)  # flatten the nested list\n",
    "    for j, ax in zip(range(plots_per_figure, df.shape[1] + 1, plots_per_figure), axes):\n",
    "        plotfunc(df.iloc[:, i:j], ax=ax)\n",
    "        i = j\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dfnum.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "featureplot(dfnum, ncols=7, nrows=4, figsize=(12, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many of the features are higly skewed with very long tails."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of these are right skewed as well. BsmtFullBath has some discrete values (number of bathrooms)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some features, such as `BsmtFinSF2`, are almost constant (blobs with long tail) as can be seen below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1, figsize=(4, 4))\n",
    "sns.distplot(dfnum.BsmtFinSF2, ax=ax)\n",
    "ax.set_title('Distribution of BsmtFinSF2')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop nearly constant features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def test_nearly_constant(series):\n",
    "    counts = series.value_counts()\n",
    "    max_val_count = max(counts)\n",
    "    other_val_count = counts.drop(counts.argmax()).sum()\n",
    "    return other_val_count / max_val_count < 0.25\n",
    "\n",
    "is_nearly_constant = dfnum.apply(test_nearly_constant)\n",
    "is_nearly_constant.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dropme = dfnum.columns[is_nearly_constant]\n",
    "dropme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def drop_constant_features(df):\n",
    "    return df.drop(df.columns[df.apply(test_nearly_constant)], axis=1)\n",
    "\n",
    "pipeline.append(drop_constant_features)\n",
    "all(dfnum.drop(dropme, axis=1) == pipeline.run())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dfnum = dfnum.drop(dropme, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Log transform the other features if they have a high skewness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using a log transformation for some of the skewed features should help, as illustrated below. We use the raw data (not the standardized one) because we need positive values for the log function (we'll standardize the transformed variables later)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1,2, figsize=(8, 4))\n",
    "sns.distplot(dfnum['LotArea'], ax=axes[0])\n",
    "sns.distplot(np.log1p(dfnum['LotArea']), ax=axes[1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use dataframe & series whenever possible for maximum flexibility (see below)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def skewtest(train, sort=True, ascending=True):\n",
    "    \"\"\"Return dataframe of zfactor and pvalue for skew test\"\"\"\n",
    "    test = sp.stats.skewtest(train)\n",
    "    zfactor = test[0]\n",
    "    pvalue = test[1]\n",
    "    df = pd.DataFrame(dict(zfactor=zfactor, pvalue=pvalue), index=train.columns)\n",
    "    if sort:\n",
    "        return df.sort_values(by='zfactor', ascending=ascending)\n",
    "    else:\n",
    "        return df\n",
    "\n",
    "skewtest(dfnum).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def is_skewed(train, min_zfactor=10, plot=False):\n",
    "    \"\"\"Return series of booleans indicating whether a column is skewed or not.\n",
    "    \"\"\"\n",
    "    sk = skewtest(train)\n",
    "    if plot:\n",
    "        plt.figure(1)\n",
    "        plt.title('Z-factor distribution from skewtest')\n",
    "        plt.xlabel('Z-factor')\n",
    "        sns.distplot(sk.zfactor)\n",
    "        plt.figure(2)\n",
    "        sk.zfactor.plot(kind='barh')\n",
    "        plt.title('Z-factor for skewtest')\n",
    "    return sk.zfactor > min_zfactor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "is_skewed(dfnum, min_zfactor=10, plot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's apply a log1p transform to all these and plot the distributions again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def transform_skewed_colums(dfnum):\n",
    "    \"\"\"\n",
    "    dfnum: dataframe to transform\n",
    "    dropme: columns to drop\n",
    "    is_skewed: iterable of length dfnum.columns indicating if a column is skewed\n",
    "    \"\"\"\n",
    "    dfnum2 = dfnum.copy()\n",
    "    skewed_colz = is_skewed(dfnum)\n",
    "    dfnum2.loc[:, skewed_colz] = dfnum2.loc[:, skewed_colz].apply(np.log1p)\n",
    "    return dfnum2\n",
    "\n",
    "pipeline.append(transform_skewed_colums)\n",
    "\n",
    "# the transformed dataset has fewer columns and we only want those\n",
    "dfnum2 = pipeline.run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dfnum.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "is_skewed(dfnum, plot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sp.stats.skewtest(dfnum2)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "zfactors2 = sp.stats.skewtest(dfnum2)[0]\n",
    "pd.Series(data=zfactors2, index=dfnum2.columns)[is_skewed(dfnum)].sort_values().plot(kind='barh')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now our originally skewed features look more symmetric. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check that the distributions are less skewed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "skewed = is_skewed(dfnum)\n",
    "skewed.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "featureplot(dfnum2.loc[:, skewed], nrows=3, ncols=6, figsize=(10,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "featureplot(dfnum2.loc[:, ~skewed], nrows=2, ncols=5, figsize=(10, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save transformed numerical data\n",
    "Use the storage magic to communicate between notebooks. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dfnum2.to_csv('transformed_dataset_dfnum2.csv', index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature selection\n",
    "We're now in a good position to identify the key numerical features. Those should be hightly correlated with the sale price."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standardize the data \n",
    "This is not in the right place. We don't normally want to standardize the data just for plotting... Although it may be the easiest way. Can we have violin plots in little individual plots instead? Sandardization is useful but probably for regression, not here!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def standardize(df):\n",
    "    return sk.preprocessing.StandardScaler().fit_transform(df)\n",
    "\n",
    "dfnum_t = dfnum.apply(standardize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dfnum_t.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nfeatures = dfnum2.columns\n",
    "target_t = logtarget.apply(standardize)\n",
    "target_t.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dfnum_t2 = dfnum2.apply(standardize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dfnum_t2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "corr = pd.DataFrame(data=dfnum_t2.loc['train',:].apply(lambda feature: sp.stats.pearsonr(feature, target_t['SalePrice'])), \n",
    "                    columns=['pearsonr'])\n",
    "corr = corr.assign(correlation=corr.applymap(lambda x: x[0]),\n",
    "                pvalue=corr.applymap(lambda x: x[1]))\n",
    "corr = corr.drop('pearsonr', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "corr.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "corr.sort_values('pvalue', ascending=False)['correlation'].plot(kind='barh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "corr.sort_values('pvalue').head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "corr.sort_values('pvalue').tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's keep only the features that have a high enough correlation with the price (correlation less than 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "min_correlation = 0.2\n",
    "key_features = corr[np.abs(corr['correlation'] > min_correlation)].sort_values(by='correlation', ascending=False).index.values\n",
    "key_features, key_features.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%store key_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
